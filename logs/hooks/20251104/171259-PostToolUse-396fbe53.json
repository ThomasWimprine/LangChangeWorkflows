{"session_id":"396fbe53-cba7-45be-89d9-14577685964b","transcript_path":"/home/thomas/.claude/projects/-home-thomas-Repositories-LangChangeWorkflows/396fbe53-cba7-45be-89d9-14577685964b.jsonl","cwd":"/home/thomas/Repositories/LangChangeWorkflows","permission_mode":"default","hook_event_name":"PostToolUse","tool_name":"Read","tool_input":{"file_path":"/home/thomas/Repositories/GitWorkflow/scripts/draft-002.py"},"tool_response":{"type":"text","file":{"filePath":"/home/thomas/Repositories/GitWorkflow/scripts/draft-002.py","content":"#!/usr/bin/env python3\nfrom __future__ import annotations\n\"\"\"\ndraft-002.py — TASK002 runner\n\nPurpose:\n- Primary mode (qa): Read TASK001 drafts, extract all questions, ask the architect, and produce a TASK002 answer per question using the strict template 002.\n- Legacy mode (steps): Runs prp-steps from a markdown file for backwards compatibility.\n\nInputs (CLI):\n- --mode: qa | steps (default: qa)\n- In qa mode:\n    - --architect-agent: Which agent answers (default: application-architect)\n    - --template: Path to TASK002 JSON template (content shape only; wrapper is enforced by the script)\n    - --include-repo-context/--no-include-repo-context\n    - --context-schema, --context-max-files\n    - --include-agent-catalog/--no-include-agent-catalog, --agent-catalog-lines\n    - --model, --max-tokens\n- In steps mode:\n    - --md: Path to markdown file containing prp-steps.\n    - --arg: Feature description used for $ARGUMENTS.\n    - --template: Path to TASK002 JSON template (enforced for generate_from_template).\n\nBehavior:\n- qa: Builds a deterministic batch to the architect with identical cached system blocks for all requests. Embeds repo context (from docs/schema.json) and aggregated TASK001 responses into system extras. Each question yields one response.\n- steps: Parses prp-steps and runs as before.\n\nOutputs:\n- qa: prp/drafts/P-###-T-002-Q-###.json using shared PRP sequence counters (P first, then T, then Q).\n- steps: prp/drafts/*-<timestamp>-<step_id>.json for wrapper-valid drafts (normalized path); tmp/panel/* and tmp/raw/* as before.\n\"\"\"\nimport argparse\nimport json\nimport os\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\nimport re\nimport glob\nimport fnmatch\nfrom typing import Any, Dict, List, Tuple\n\nfrom dotenv import load_dotenv, find_dotenv\nimport anthropic\n\n# Standalone constants and helpers\nMODEL_ID = \"claude-sonnet-4-5\"\nAGENT_DIRS = [Path(os.path.expanduser(\"~/.claude/agents\"))]\n# Optional: support extra agent directories via env var\n_extra_dirs = os.getenv(\"CLAUDE_AGENT_DIRS\", \"\").strip()\nif _extra_dirs:\n    for _d in _extra_dirs.split(\":\"):\n        if _d.strip():\n            AGENT_DIRS.append(Path(_d.strip()))\n\n\ndef load_agent_text(name: str) -> str:\n    \"\"\"Load agent system prompt text from registry directories.\n\n    Search order is AGENT_DIRS; file is expected to be '<name>.md'.\n    Raises FileNotFoundError if not found.\n    \"\"\"\n    fname = f\"{name}.md\"\n    for base in AGENT_DIRS:\n        p = base / fname\n        if p.exists():\n            return p.read_text(encoding=\"utf-8\", errors=\"replace\")\n    raise FileNotFoundError(f\"Agent file not found for '{name}' in: {', '.join(str(d) for d in AGENT_DIRS)}\")\n\n\ndef _ensure_parent_dir(path_str: str) -> None:\n    \"\"\"Ensure parent directory of the path exists.\"\"\"\n    p = Path(path_str)\n    p.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef _slugify(text: str) -> str:\n    \"\"\"Make a filesystem-friendly slug from free text.\"\"\"\n    txt = text.lower().strip()\n    txt = re.sub(r\"[^a-z0-9\\-\\s]\", \"\", txt)\n    txt = re.sub(r\"\\s+\", \"-\", txt)\n    txt = re.sub(r\"-+\", \"-\", txt)\n    return txt or \"draft\"\n\n\nclass SpecError(Exception):\n    pass\n\n\nclass Step:\n    def __init__(self, id: str, agent: str, action: str, inputs: Dict[str, Any], outputs: Dict[str, Any]):\n        self.id = id\n        self.agent = agent\n        self.action = action\n        self.inputs = inputs\n        self.outputs = outputs\n\n\n# --- PRP Sequence Register (shared with TASK001) ---\ndef _seq_path() -> Path:\n    return Path(\"prp/prp_seq.json\")\n\n\ndef _read_seq() -> dict:\n    p = _seq_path()\n    if not p.exists():\n        p.parent.mkdir(parents=True, exist_ok=True)\n        data = {\"P\": 0, \"Q\": {\"002\": 0}}\n        p.write_text(json.dumps(data, separators=(\",\", \":\")), encoding=\"utf-8\")\n        return data\n    try:\n        return json.loads(p.read_text(encoding=\"utf-8\"))\n    except Exception:\n        return {\"P\": 0, \"Q\": {\"002\": 0}}\n\n\ndef _write_seq(data: dict) -> None:\n    p = _seq_path()\n    tmp = p.with_suffix(\".tmp\")\n    p.parent.mkdir(parents=True, exist_ok=True)\n    tmp.write_text(json.dumps(data, separators=(\",\", \":\")), encoding=\"utf-8\")\n    tmp.replace(p)\n\n\ndef _next_P() -> int:\n    data = _read_seq()\n    data[\"P\"] = int(data.get(\"P\", 0)) + 1\n    _write_seq(data)\n    return data[\"P\"]\n\n\ndef _next_Q(task: str = \"002\") -> int:\n    data = _read_seq()\n    q = data.get(\"Q\") or {}\n    curr = int((q.get(task) or 0)) + 1\n    q[task] = curr\n    data[\"Q\"] = q\n    _write_seq(data)\n    return curr\n\n\ndef _load_repo_context_from_schema(\n    schema_path: Path,\n    workspace_root: Path,\n    *,\n    max_files: int | None = None,\n) -> Tuple[str, str] | None:\n    \"\"\"Build a deterministic repo context JSON with FULL FILE CONTENTS per schema.\n\n    Returns (context_json, ttl_str) or None if schema missing/invalid.\n    The JSON shape is { \"repo_context\": [{\"path\": str, \"size\": int, \"content\": str}], \"schema_source\": str }.\n    All files matching schema include/exclude/extensions are included, sorted by path, and truncated ONLY by schema.maxFileKB.\n    If max_files is provided and > 0, at most that many files are included (in sorted order). If None or <=0, include all.\n    \"\"\"\n    try:\n        if not schema_path.exists():\n            return None\n        schema = json.loads(schema_path.read_text(encoding=\"utf-8\"))\n    except Exception:\n        return None\n\n    include: list[str] = schema.get(\"include\", []) or []\n    exclude: list[str] = schema.get(\"exclude\", []) or []\n    extensions: list[str] = schema.get(\"extensions\", []) or []\n    max_kb: int = int(schema.get(\"maxFileKB\", 256) or 256)\n    follow_symlinks: bool = bool(schema.get(\"followSymlinks\", False))\n    ttl_hours: int = int(schema.get(\"cacheTTLHours\", 24) or 24)\n\n    candidates: set[Path] = set()\n    for pat in include:\n        abs_pat = str(workspace_root / pat)\n        for m in glob.glob(abs_pat, recursive=True):\n            p = Path(m)\n            if p.is_dir():\n                continue\n            if not follow_symlinks and p.is_symlink():\n                continue\n            candidates.add(p)\n\n    def _is_excluded(p: Path) -> bool:\n        rel = str(p.relative_to(workspace_root)) if p.is_absolute() else str(p)\n        for pat in exclude:\n            if fnmatch.fnmatch(rel, pat):\n                return True\n        return False\n\n    allowed_ext = set(e.lower() for e in extensions if isinstance(e, str))\n    items: list[dict[str, Any]] = []\n    max_bytes = max(1, max_kb) * 1024\n\n    for p in sorted(candidates, key=lambda q: str(q)):\n        try:\n            rel = str(p.relative_to(workspace_root))\n        except Exception:\n            continue\n        if _is_excluded(p):\n            continue\n        if allowed_ext:\n            if p.suffix.lower() not in allowed_ext:\n                continue\n        try:\n            sz = p.stat().st_size\n        except Exception:\n            continue\n        if sz > max_bytes:\n            continue\n        try:\n            text = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n        except Exception:\n            try:\n                text = p.read_bytes().decode(\"utf-8\", errors=\"replace\")\n            except Exception:\n                continue\n        items.append({\"path\": rel, \"size\": sz, \"content\": text})\n        if isinstance(max_files, int) and max_files > 0 and len(items) >= max_files:\n            break\n\n    payload = {\n        \"repo_context\": items,\n        \"schema_source\": str(schema_path.relative_to(workspace_root)) if schema_path.is_relative_to(workspace_root) else str(schema_path),\n    }\n    context_json = json.dumps(payload, indent=2, sort_keys=True, ensure_ascii=False)\n    ttl = f\"{max(1, ttl_hours)}h\"\n    return context_json, ttl\n\n\ndef _collect_task001_data() -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"Return (responses, questions) from TASK001 drafts in prp/drafts.\n\n    Supports both new naming (P-*-T-001.json) and legacy timestamped names (*task001*.json).\n\n    responses: [{\"file\": str, \"content\": dict}]\n    questions: [{\"question\": str, \"agent\": str, \"source_file\": str}]\n    \"\"\"\n    draft_dir = Path(\"prp/drafts\")\n    seen_paths: set[str] = set()\n    drafts: List[Path] = []\n    # New naming (supports both P-###-T-001.json and P-###-T-001-{agent}.json)\n    for p in sorted(draft_dir.glob(\"P-*-T-001*.json\"), key=lambda q: str(q)):\n        sp = str(p)\n        if sp not in seen_paths:\n            seen_paths.add(sp)\n            drafts.append(p)\n    # Legacy naming that includes task001 in the filename\n    for p in sorted(draft_dir.glob(\"*task001*.json\"), key=lambda q: str(q)):\n        sp = str(p)\n        if sp not in seen_paths:\n            seen_paths.add(sp)\n            drafts.append(p)\n    responses: List[Dict[str, Any]] = []\n    questions: List[Dict[str, Any]] = []\n    for p in drafts:\n        try:\n            root = json.loads(p.read_text(encoding=\"utf-8\"))\n        except Exception:\n            continue\n        if not isinstance(root, dict):\n            continue\n        content = root.get(\"content\") if isinstance(root.get(\"content\"), dict) else None\n        if not isinstance(content, dict):\n            continue\n        # Heuristic: consider only TASK001-like content shapes\n        is_task001_shape = (\n            (\"atomicity\" in content or \"proposed_tasks\" in content) and\n            (isinstance(content.get(\"Questions\"), list) or isinstance(content.get(\"questions\"), list))\n        )\n        if not is_task001_shape:\n            continue\n        responses.append({\"file\": p.name, \"content\": content})\n        qlist = None\n        # Support both canonical and lowercase field names\n        if isinstance(content.get(\"Questions\"), list):\n            qlist = content.get(\"Questions\")\n        elif isinstance(content.get(\"questions\"), list):\n            qlist = content.get(\"questions\")\n        if isinstance(qlist, list):\n            for q in qlist:\n                if isinstance(q, dict) and isinstance(q.get(\"question\"), str):\n                    questions.append({\n                        \"question\": q.get(\"question\"),\n                        \"agent\": (q.get(\"agent\") if isinstance(q.get(\"agent\"), str) else \"\"),\n                        \"source_file\": p.name,\n                    })\n    # Deterministic order across runs\n    questions.sort(key=lambda d: (d.get(\"agent\") or \"\", d.get(\"question\") or \"\", d.get(\"source_file\") or \"\"))\n    responses.sort(key=lambda d: d.get(\"file\") or \"\")\n    return responses, questions\n\n\ndef _dedup_questions_via_model(client: Any, model: str, max_tokens: int, questions: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n    \"\"\"Ask the model to deduplicate questions. Returns (dedup_list, raw_payload).\n\n    Input questions are a list of {question, agent, source_file}. The model should group semantic duplicates and\n    return JSON with the following shape:\n    {\n      \"questions\": [\n        {\"question\": str, \"agents\": [str], \"source_files\": [str]}\n      ]\n    }\n    \"\"\"\n    # Build deterministic user instruction with input JSON\n    src = sorted([\n        {\n            \"question\": (q.get(\"question\") or \"\").strip(),\n            \"agent\": (q.get(\"agent\") or \"\").strip(),\n            \"source_file\": (q.get(\"source_file\") or \"\").strip(),\n        }\n    for q in questions\n    if isinstance(q, dict) and isinstance(q.get(\"question\"), str) and (q.get(\"question\") or \"\").strip()\n    ], key=lambda d: (d[\"question\"], d[\"agent\"], d[\"source_file\"]))\n\n    input_json = json.dumps({\"questions\": src}, indent=2, sort_keys=True, ensure_ascii=False)\n    user_text = (\n        \"Task: Deduplicate semantically equivalent questions.\\n\\n\"\n        \"Instructions:\\n\"\n        \"- Normalize whitespace and punctuation; treat questions with the same meaning as duplicates.\\n\"\n        \"- Produce a canonical phrasing for each group.\\n\"\n        \"- Aggregate all agents and source_files that raised the duplicate question.\\n\\n\"\n        \"Input JSON:\\n\" + input_json + \"\\n\\n\"\n        \"Output JSON schema (return JSON only):\\n\"\n        \"{\\n\"\n        \"  \\\"questions\\\": [\\n\"\n        \"    { \\\"question\\\": \\\"...\\\", \\\"agents\\\": [\\\"...\\\"], \\\"source_files\\\": [\\\"...\\\"] }\\n\"\n        \"  ]\\n\"\n        \"}\\n\"\n    )\n\n    # Use default system (standard model behavior)\n    req = {\n        \"custom_id\": \"dedup-questions\",\n        \"params\": {\n            \"model\": model,\n            \"max_tokens\": int(max_tokens),\n            \"temperature\": 0.2,\n            \"messages\": [\n                {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}\n            ],\n        },\n    }\n\n    batch = client.messages.batches.create(requests=[req])\n    while True:\n        b = client.messages.batches.retrieve(batch.id)\n        if b.processing_status in (\"ended\", \"failed\", \"expired\"):\n            break\n        time.sleep(1)\n    items = list(client.messages.batches.results(batch.id))\n    if not items:\n        return [], {}\n    blocks = _extract_text_blocks_from_result(items[0])\n    combined = \"\\n\\n\".join(blocks) if blocks else \"\"\n    # Save raw for diagnostics\n    try:\n        raw_path = Path(\"tmp/raw/dedup-questions.txt\")\n        _ensure_parent_dir(str(raw_path))\n        raw_path.write_text(combined or \"\", encoding=\"utf-8\")\n    except Exception:\n        pass\n    payload = _extract_first_json_object(combined) or _extract_fenced_json(combined)\n    if isinstance(payload, str):\n        try:\n            payload = json.loads(payload)\n        except Exception:\n            pass\n    # Normalize various shapes into a list of entries\n    def _normalize_list(lst: Any) -> List[Dict[str, Any]]:\n        out: List[Dict[str, Any]] = []\n        if not isinstance(lst, list):\n            return out\n        for e in lst:\n            if isinstance(e, str):\n                q = e.strip()\n                if q:\n                    out.append({\"question\": q, \"agents\": [], \"source_files\": []})\n            elif isinstance(e, dict):\n                q = (e.get(\"question\") or \"\").strip()\n                if not q:\n                    # Some models may use a different key like 'q'\n                    q = (e.get(\"q\") or \"\").strip()\n                if q:\n                    agents = [a.strip() for a in (e.get(\"agents\") or []) if isinstance(a, str) and a.strip()]\n                    srcs = [s.strip() for s in (e.get(\"source_files\") or []) if isinstance(s, str) and s.strip()]\n                    out.append({\"question\": q, \"agents\": sorted(set(agents)), \"source_files\": sorted(set(srcs))})\n        return out\n\n    dedup: List[Dict[str, Any]] = []\n    if isinstance(payload, dict):\n        out_list = payload.get(\"questions\")\n        dedup = _normalize_list(out_list)\n    elif isinstance(payload, list):\n        dedup = _normalize_list(payload)\n    else:\n        dedup = []\n    diag: Dict[str, Any] = {}\n    if isinstance(payload, dict):\n        diag = payload\n    elif isinstance(payload, list):\n        diag = {\"list_payload\": payload}\n    return dedup, diag\n\n\ndef parse_prp_steps(path: str):\n    \"\"\"Minimal parser to extract steps from a markdown file containing a fenced yaml block.\n\n    Expects a block like:\n    ```yaml\n    prp-steps:\n      command: run\n      args: { feature_description: \"...\" }\n      agents: [a,b]\n      steps:\n        - id: create_draft\n          agent: project-manager\n          action: generate_from_template\n          inputs: { template: templates/prp/draft-prp-001.json, feature: $ARGUMENTS }\n          outputs: { draft_file: \"{slug}-{timestamp}-{id}.json\" }\n    ```\n    \"\"\"\n    import yaml\n    text = Path(path).read_text(encoding=\"utf-8\")\n    # find first fenced yaml block\n    import re as _re\n    m = _re.search(r\"```yaml\\s*(.*?)```\", text, _re.DOTALL | _re.IGNORECASE)\n    if not m:\n        raise SpecError(\"No yaml block found in markdown\")\n    data = yaml.safe_load(m.group(1))\n    root = data.get(\"prp-steps\") if isinstance(data, dict) else None\n    if not isinstance(root, dict):\n        raise SpecError(\"Missing prp-steps root\")\n    steps = root.get(\"steps\") or []\n    out = []\n    for s in steps:\n        if not isinstance(s, dict):\n            continue\n        out.append(Step(\n            id=str(s.get(\"id\")),\n            agent=str(s.get(\"agent\")),\n            action=str(s.get(\"action\")),\n            inputs=s.get(\"inputs\") or {},\n            outputs=s.get(\"outputs\") or {},\n        ))\n    class Spec:\n        def __init__(self, steps):\n            self.steps = steps\n    return Spec(out)\n\n\ndef _resolve_value(val: Any, runtime_args: Dict[str, str]) -> Any:\n    \"\"\"Resolve special placeholders from runtime arguments.\"\"\"\n    if not isinstance(val, str):\n        return val\n    if val == \"$ARGUMENTS\" or val == \"$ARGUMENTS/feature_description\":\n        return runtime_args.get(\"feature_description\")\n    return val\n\n\ndef _resolve_mapping(d: Dict[str, Any], runtime_args: Dict[str, str]) -> Dict[str, Any]:\n    \"\"\"Apply _resolve_value across a mapping.\"\"\"\n    return {k: _resolve_value(v, runtime_args) for k, v in d.items()}\n\n\ndef build_user_instruction_for_step(step: Step, resolved_inputs: Dict[str, Any]) -> str:\n    \"\"\"Construct the user prompt for the given step and resolved inputs.\n\n    - For generate_from_template: embeds the TASK002 JSON template text.\n    - For run_script: includes the file content for analysis.\n    \"\"\"\n    if step.action == \"generate_from_template\":\n        template_path = str(resolved_inputs.get(\"template\", \"\"))\n        feature_desc = str(resolved_inputs.get(\"feature\", \"\"))\n        prefix = str(resolved_inputs.get(\"_prompt_text\", \"\")).strip()\n        t = Path(template_path).read_text(encoding=\"utf-8\") if Path(template_path).exists() else \"\"\n        if template_path.lower().endswith(\".json\"):\n            core = (\n                \"Task: STRICT TEMPLATE COMPLIANCE — Fill the provided JSON template EXACTLY.\\n\\n\"\n                \"Rules:\\n\"\n                \"- Do not add or remove keys\\n\"\n                \"- Preserve key order and nested structure\\n\"\n                \"- Replace only placeholder/example values\\n\"\n                \"- Keep the PRP atomic (single objective, ≤2 affected components)\\n\\n\"\n                f\"Feature Description:\\n{feature_desc}\\n\\n\"\n                f\"JSON Template (copy structure exactly):\\n{t}\\n\\n\"\n                \"Output: Return JSON only with this shape:\\n\"\n                \"{\\n\"\n                \"  \\\"outputs\\\": { \\\"draft_file\\\": \\\"<suggested-path>.json\\\" },\\n\"\n                \"  \\\"content\\\": <the filled JSON object>\\n\"\n                \"}\\n\"\n                \"Do not wrap the content object as a string. Do not include commentary outside JSON.\"\n            )\n            return ((\"Task Prompt (verbatim, read fully):\\n\" + prefix + \"\\n\\n\") if prefix else \"\") + core\n        else:\n            core = (\n                \"Task: STRICT TEMPLATE COMPLIANCE — Generate a DRAFT PRP by COPYING the provided template’s headings and section order EXACTLY.\\n\\n\"\n                f\"Feature Description: \\n{feature_desc}\\n\\n\"\n                f\"Template Content (copy structure exactly):\\n{t}\\n\\n\"\n                \"Output: Return JSON only:\\n\"\n                \"{\\n\"\n                \"  \\\"outputs\\\": { \\\"draft_file\\\": \\\"<suggested-path>.md\\\" },\\n\"\n                \"  \\\"content\\\": \\\"<the generated markdown content>\\\"\\n\"\n                \"}\\n\"\n            )\n            return ((\"Task Prompt (verbatim, read fully):\\n\" + prefix + \"\\n\\n\") if prefix else \"\") + core\n    if step.action == \"run_script\":\n        file_path = str(resolved_inputs.get(\"file\", \"\"))\n        file_text = Path(file_path).read_text(encoding=\"utf-8\") if Path(file_path).exists() else \"\"\n        return (\n            \"Task: Analyze the provided markdown draft for size and splitting recommendations.\\n\\n\"\n            f\"Draft Path: {file_path}\\n\"\n            f\"Draft Content:\\n{file_text}\\n\\n\"\n            \"Compute:\\n- Total character count (include markdown, code, whitespace)\\n\"\n            \"- If <= 5000: status = under_limit\\n\"\n            \"- If > 5000: status = over_limit, estimate number of atomic PRPs and propose split points\\n\\n\"\n            \"Output: Return JSON only with keys:\\n\"\n            \"{\\n\"\n            '  \"report\": {\\n'\n            '    \"total_chars\": <int>,\\n'\n            '    \"status\": \"under_limit\" | \"over_limit\",\\n'\n            '    \"estimated_prps\": <int>,\\n'\n            '    \"suggested_splits\": [\"<section or boundary>\"],\\n'\n            '    \"notes\": \"<brief>\"\\n'\n            \"  }\\n\"\n            \"}\\n\"\n        )\n    return f\"Unsupported action '{step.action}'. Provide no output.\"\n\n\ndef _build_task002_user_instruction(template_path: str, architect: str, question_text: str, q_id: int) -> str:\n        \"\"\"Build the user prompt for TASK002 QA per question, embedding the template JSON.\n\n        Enforces a strict wrapper:\n        {\n            \"outputs\": { \"draft_file\": \"<suggested>.json\" },\n            \"content\": { ...filled template 002... }\n        }\n        \"\"\"\n        t = Path(template_path).read_text(encoding=\"utf-8\") if Path(template_path).exists() else \"\"\n        qid_str = f\"T-002-Q-{q_id:03d}\"\n        example = (\n            '{\\n'\n            '  \"outputs\": { \"draft_file\": \"P-XXX-T-002-Q-YYY.json\" },\\n'\n            '  \"content\": {\\n'\n            f'    \"agent_name\": \"{architect}\",\\n'\n            f'    \"id\": \"{qid_str}\",\\n'\n            f'    \"question\": {json.dumps(question_text)},\\n'\n            '    \"answer\": \"...\",\\n'\n            '    \"citation\": [\"...\"]\\n'\n            '  }\\n'\n            '}'\n        )\n        return (\n            \"Task: STRICT TEMPLATE COMPLIANCE — Answer the question using TASK002 template EXACTLY.\\n\\n\"\n            \"Rules:\\n\"\n            \"- Return JSON only, no prose\\n\"\n            \"- Do not add or remove keys\\n\"\n            \"- Preserve key order and structure\\n\"\n            \"- Replace only placeholder/example values\\n\"\n            \"- Copy the question text exactly into content.question\\n\"\n            \"- Include citations to repo_context paths or entries in TASK001 RESPONSES\\n\\n\"\n            f\"Question:\\n{question_text}\\n\\n\"\n            f\"Constraints:\\n- agent_name MUST be '{architect}'\\n- id MUST be '{qid_str}'\\n\\n\"\n            f\"JSON Template (copy structure exactly):\\n{t}\\n\\n\"\n            \"Output wrapper (start your response with '{\\\"outputs\\\"'):\\n\"\n            f\"{example}\\n\"\n        )\n\n\ndef _extract_text_blocks_from_result(item) -> list[str]:\n    \"\"\"Normalize Anthropic batch result item into a list of text blocks.\"\"\"\n    if isinstance(item, str):\n        try:\n            obj = json.loads(item)\n        except Exception:\n            return [item]\n        result = obj.get(\"result\") if isinstance(obj, dict) else None\n        message = (result or {}).get(\"message\") if isinstance(result, dict) else None\n        content = (message or {}).get(\"content\", []) if isinstance(message, dict) else []\n        blocks: list[str] = []\n        for c in content:\n            if isinstance(c, dict) and c.get(\"type\") == \"text\":\n                blocks.append(c.get(\"text\", \"\"))\n        return blocks\n    try:\n        result = getattr(item, \"result\", None)\n        message = getattr(result, \"message\", None)\n        content = getattr(message, \"content\", [])\n        blocks: list[str] = []\n        for c in content:\n            if hasattr(c, \"type\") and getattr(c, \"type\") == \"text\":\n                blocks.append(getattr(c, \"text\", \"\"))\n            elif isinstance(c, dict) and c.get(\"type\") == \"text\":\n                blocks.append(c.get(\"text\", \"\"))\n        return blocks\n    except Exception:\n        return []\n\n\ndef _get_custom_id(item) -> str | None:\n    \"\"\"Extract custom_id from batch item, if present.\"\"\"\n    try:\n        cid = getattr(item, \"custom_id\", None)\n        if cid:\n            return cid\n    except Exception:\n        pass\n    if isinstance(item, str):\n        try:\n            obj = json.loads(item)\n            return obj.get(\"custom_id\")\n        except Exception:\n            return None\n    return None\n\n\ndef _extract_first_json_object(s: str):\n    \"\"\"Return the first parseable JSON object found in the string, else None.\"\"\"\n    start = s.find(\"{\")\n    while start != -1:\n        depth = 0\n        for i in range(start, len(s)):\n            ch = s[i]\n            if ch == \"{\":\n                depth += 1\n            elif ch == \"}\":\n                depth -= 1\n                if depth == 0:\n                    candidate = s[start:i+1]\n                    try:\n                        return json.loads(candidate)\n                    except Exception:\n                        break\n        start = s.find(\"{\", start + 1)\n    return None\n\n\ndef _extract_fenced_json(s: str):\n    \"\"\"Return JSON parsed from a ```json fenced block, or the raw block on parse failure.\"\"\"\n    import re as _re\n    pattern = _re.compile(r\"```json\\s*(.*?)```\", _re.DOTALL | _re.IGNORECASE)\n    m = pattern.search(s)\n    if not m:\n        return None\n    block = m.group(1).strip()\n    try:\n        return json.loads(block)\n    except Exception:\n        return block\n\n\ndef _normalize_draft_path(suggested: str, slug: str, step_id: str, ts: str, ext_mode: str) -> str:\n    \"\"\"Normalize suggested draft path into prp/drafts with timestamp and step id.\n\n    ext_mode controls extension behavior; when json, ensures .json extension and removes any .json in stem.\n    \"\"\"\n    base = Path(str(suggested)).name.strip()\n    if not base:\n        base = slug\n    if base.startswith(\".\") and base.count(\".\") == 1:\n        stem, right_ext = base, \"\"\n    else:\n        stem = base.rsplit(\".\", 1)[0] if \".\" in base else base\n        right_ext = \".\" + base.rsplit(\".\", 1)[1] if \".\" in base else \"\"\n    if ext_mode == \"preserve\" and right_ext:\n        final_ext = right_ext\n    elif ext_mode == \"mdx\":\n        final_ext = \".mdx\"\n    elif ext_mode == \"json\":\n        final_ext = \".json\"\n    else:\n        final_ext = \".md\"\n    if ext_mode not in (\"preserve\", \"json\"):\n        try:\n            stem = re.sub(r\"\\.json\\b\", \"\", stem, flags=re.IGNORECASE)\n        except Exception:\n            stem = stem.replace(\".json\", \"\")\n    has_ts = ts in stem or bool(re.search(r\"\\b\\d{8}-\\d{6}\\b\", stem))\n    has_step = step_id in stem\n    new_stem = stem\n    if not has_ts:\n        new_stem = f\"{new_stem}-{ts}\"\n    if not has_step:\n        new_stem = f\"{new_stem}-{step_id}\"\n    final_name = f\"{new_stem}{final_ext}\"\n    return str(Path(\"prp/drafts\") / final_name)\n\n\ndef main() -> int:\n    \"\"\"CLI entrypoint for TASK002.\n\n    Returns non-zero on configuration or API errors; 0 on success.\n    \"\"\"\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--mode\", choices=[\"qa\", \"steps\"], default=\"qa\")\n    # qa mode\n    ap.add_argument(\"--architect-agent\", default=\"application-architect\")\n    ap.add_argument(\"--model\", default=MODEL_ID)\n    ap.add_argument(\"--max-tokens\", type=int, default=8192)\n    ap.add_argument(\"--template\", default=\"templates/prp/draft-prp-002.json\", help=\"Template for TASK002 content shape (wrapper enforced by script)\")\n    ap.add_argument(\"--include-repo-context\", dest=\"include_repo_context\", action=\"store_true\", default=True)\n    ap.add_argument(\"--no-include-repo-context\", dest=\"include_repo_context\", action=\"store_false\")\n    ap.add_argument(\"--context-schema\", default=\"docs/schema.json\")\n    ap.add_argument(\"--context-max-files\", type=int, default=0)\n    ap.add_argument(\"--include-agent-catalog\", dest=\"include_agent_catalog\", action=\"store_true\", default=True)\n    ap.add_argument(\"--no-include-agent-catalog\", dest=\"include_agent_catalog\", action=\"store_false\")\n    ap.add_argument(\"--agent-catalog-lines\", type=int, default=7)\n    ap.add_argument(\"--limit-questions\", type=int, default=0, help=\"If >0, process at most this many questions (deterministic order)\")\n    ap.add_argument(\"--filter-agent\", default=\"\", help=\"If set, include only questions where agent equals this (case-insensitive match)\")\n    ap.add_argument(\"--filter-contains\", default=\"\", help=\"If set, include only questions whose text contains this substring (case-insensitive)\")\n    ap.add_argument(\"--dedup-max-tokens\", type=int, default=2048, help=\"Max tokens for the dedup model pass (default 2048)\")\n    # steps mode\n    ap.add_argument(\"--md\", default=\"prompts/prp/draft-prp-002.md\", help=\"Markdown file with prp-steps (steps mode)\")\n    ap.add_argument(\"--arg\", dest=\"feature_description\", required=False, help=\"$ARGUMENTS value for steps mode\")\n    ap.add_argument(\"--prompt\", default=\"prompts/prp/draft-prp-002.md\", help=\"Optional prompt file (steps mode)\")\n    args = ap.parse_args()\n\n    try:\n        load_dotenv(find_dotenv(usecwd=True), override=False)\n    except Exception:\n        pass\n\n    api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        print(\"ERROR: Set ANTHROPIC_API_KEY in environment or .env\")\n        return 2\n\n    client = anthropic.Anthropic(api_key=api_key)\n\n    if args.mode == \"qa\":\n        # Build shared system extras once for caching determinism\n        system_extras: List[dict] = []\n        # Architect system prompt\n        try:\n            architect_text = load_agent_text(args.architect_agent)\n        except FileNotFoundError as e:\n            print(f\"ERROR: {e}\")\n            return 2\n        system_extras.append({\"type\": \"text\", \"text\": architect_text, \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}})\n\n        # Optional agent catalog (deterministic)\n        if args.include_agent_catalog:\n            catalog = []\n            for base in AGENT_DIRS:\n                try:\n                    files = sorted(Path(base).glob(\"*.md\"), key=lambda q: q.stem)\n                    for p in files:\n                        stem = p.stem\n                        text = p.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines()\n                        head = text[: max(0, int(args.agent_catalog_lines))]\n                        catalog.append({\"id\": stem, \"summary\": \"\\n\".join(head).strip()})\n                except Exception:\n                    continue\n            catalog = sorted(catalog, key=lambda x: x[\"id\"]) \n            catalog_json = json.dumps({\"available_agents\": catalog}, indent=2, sort_keys=True, ensure_ascii=False)\n            system_extras.append({\"type\": \"text\", \"text\": \"KNOWN AGENTS CATALOG (JSON):\\n\" + catalog_json, \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}})\n\n        # Repo context\n        if args.include_repo_context:\n            ctx = _load_repo_context_from_schema(\n                schema_path=Path(args.context_schema),\n                workspace_root=Path.cwd(),\n                max_files=(args.context_max_files if args.context_max_files > 0 else None),\n            )\n            if ctx:\n                context_json, _ttl = ctx\n                system_extras.append({\"type\": \"text\", \"text\": \"REPO CONTEXT INDEX (JSON):\\n\" + context_json, \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}})\n\n        # Aggregate TASK001 responses and questions\n        responses, questions = _collect_task001_data()\n        # Optional filtering\n        filt_agent = (args.filter_agent or \"\").strip().lower()\n        filt_sub = (args.filter_contains or \"\").strip().lower()\n        if filt_agent or filt_sub:\n            qf = []\n            for q in questions:\n                agent = (q.get(\"agent\") or \"\").strip().lower()\n                text = (q.get(\"question\") or \"\").strip().lower()\n                if filt_agent and agent != filt_agent:\n                    continue\n                if filt_sub and filt_sub not in text:\n                    continue\n                qf.append(q)\n            questions = qf\n        # Optional limit\n        if isinstance(args.limit_questions, int) and args.limit_questions > 0:\n            questions = questions[: args.limit_questions]\n\n        if not questions:\n            print(\"No questions found from TASK001 drafts.\")\n            return 0\n        print(f\"Selected questions before dedup: {len(questions)}\")\n\n        # Dedup questions via a simple model pass (no special system context)\n        dedup_list, dedup_payload = _dedup_questions_via_model(\n            client=client, model=args.model, max_tokens=args.dedup_max_tokens, questions=questions\n        )\n        # Local fallback dedup if model returns empty\n        if not dedup_list:\n            seen = {}\n            import re as _re\n            def _norm(s: str) -> str:\n                s = (s or \"\").strip().lower()\n                s = _re.sub(r\"\\s+\", \" \", s)\n                s = _re.sub(r\"[\\.?!,;:]+$\", \"\", s)\n                return s\n            for q in questions:\n                qt = _norm(q.get(\"question\") or \"\")\n                if not qt:\n                    continue\n                entry = seen.setdefault(qt, {\"question\": q.get(\"question\") or \"\", \"agents\": set(), \"source_files\": set()})\n                a = (q.get(\"agent\") or \"\").strip()\n                if a:\n                    entry[\"agents\"].add(a)\n                s = (q.get(\"source_file\") or \"\").strip()\n                if s:\n                    entry[\"source_files\"].add(s)\n            dedup_list = [\n                {\"question\": v[\"question\"], \"agents\": sorted(v[\"agents\"]), \"source_files\": sorted(v[\"source_files\"]) }\n                for v in seen.values()\n            ]\n        # Save single dedup file under prp/drafts with next P and marker T-002-DEDUPE\n        P_d = _next_P()\n        dedup_path = Path(\"prp/drafts\") / f\"P-{P_d:03d}-T-002-DEDUPE.json\"\n        dedup_path.parent.mkdir(parents=True, exist_ok=True)\n        to_save = {\"outputs\": {\"draft_file\": str(dedup_path)}, \"content\": {\"questions\": dedup_list}, \"diagnostics\": dedup_payload if isinstance(dedup_payload, dict) else {}}\n        dedup_path.write_text(json.dumps(to_save, indent=2), encoding=\"utf-8\")\n        print(f\"Saved dedup questions -> {dedup_path}\")\n        # Use dedupbed list for QA\n        questions = [{\"question\": e.get(\"question\"), \"agent\": \",\".join(e.get(\"agents\", [])), \"source_file\": \",\".join(e.get(\"source_files\", []))} for e in dedup_list]\n        print(f\"Questions after dedup: {len(questions)}\")\n        prp_json = json.dumps({\"task001_responses\": responses}, indent=2, sort_keys=True, ensure_ascii=False)\n        system_extras.append({\"type\": \"text\", \"text\": \"TASK001 RESPONSES (JSON):\\n\" + prp_json, \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}})\n\n        # Build requests, one per question, with identical system blocks across all\n        tpl_path = args.template\n        requests = []\n        cid_to_q: Dict[str, int] = {}\n        for q_item in questions:\n            qn = str(q_item.get(\"question\") or \"\").strip()\n            if not qn:\n                continue\n            Qnum = _next_Q(\"002\")\n            qid_str = f\"T-002-Q-{Qnum:03d}\"\n            user_text = _build_task002_user_instruction(tpl_path, args.architect_agent, qn, Qnum)\n            req = {\n                \"custom_id\": f\"qa-Q-{Qnum:03d}\",\n                \"params\": {\n                    \"model\": args.model,\n                    \"max_tokens\": int(args.max_tokens),\n                    \"temperature\": 0.2,\n                    \"system\": system_extras,\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}\n                    ],\n                },\n            }\n            cid_to_q[req[\"custom_id\"]] = Qnum\n            requests.append(req)\n\n        if not requests:\n            print(\"No valid questions to process.\")\n            return 0\n\n        batch = client.messages.batches.create(requests=requests)\n        print(f\"batch_id={batch.id} status={batch.processing_status} count={len(requests)}\")\n        while True:\n            b = client.messages.batches.retrieve(batch.id)\n            print(f\"poll: status={b.processing_status}\")\n            if b.processing_status in (\"ended\", \"failed\", \"expired\"):\n                break\n            time.sleep(2)\n        items = list(client.messages.batches.results(batch.id))\n        print(f\"results_count={len(items)}\")\n        if not items:\n            print(\"No results returned.\")\n            return 3\n\n        for it in items:\n            blocks = _extract_text_blocks_from_result(it)\n            cid = _get_custom_id(it) or \"\"\n            if not blocks:\n                print(f\"WARN: no text blocks for {cid or 'unknown'}; raw=\\n{it}\")\n                continue\n            combined = \"\\n\\n\".join(blocks)\n            payload = _extract_first_json_object(combined) or _extract_fenced_json(combined)\n            if isinstance(payload, str):\n                try:\n                    payload = json.loads(payload)\n                except Exception:\n                    pass\n            qnum = cid_to_q.get(cid)\n            if not isinstance(payload, dict):\n                # save raw\n                label = f\"Q-{qnum:03d}\" if isinstance(qnum, int) else \"unknown\"\n                raw_out = f\"tmp/raw/task002-{label}.txt\"\n                _ensure_parent_dir(raw_out)\n                Path(raw_out).write_text(combined, encoding=\"utf-8\")\n                print(f\"Saved raw -> {raw_out}\")\n                continue\n\n            # Coerce content-only responses into wrapper if possible\n            outs = payload.get(\"outputs\") if isinstance(payload.get(\"outputs\"), dict) else None\n            content = payload.get(\"content\") if isinstance(payload.get(\"content\"), dict) else None\n            if content is None:\n                looks_like_content = (\n                    isinstance(payload.get(\"answer\"), str)\n                    or isinstance(payload.get(\"agent_name\"), str)\n                    or isinstance(payload.get(\"id\"), str)\n                )\n                if looks_like_content:\n                    content = payload\n                    payload = {\"content\": content}\n            if not isinstance(outs, dict):\n                outs = {}\n                payload[\"outputs\"] = outs\n            if not isinstance(qnum, int):\n                print(f\"WARN: missing Q mapping for {cid}\")\n                continue\n            # Enforce content fields\n            try:\n                if isinstance(content, dict):\n                    content.setdefault(\"agent_name\", args.architect_agent)\n                    content[\"agent_name\"] = args.architect_agent\n                    content.setdefault(\"id\", f\"T-002-Q-{qnum:03d}\")\n                    content[\"id\"] = f\"T-002-Q-{qnum:03d}\"\n                    # Ensure question is present and matches qn when possible\n                    qv = content.get(\"question\")\n                    if not isinstance(qv, str) or not qv.strip():\n                        # Try to recover from the user prompt context\n                        # We cannot access the original text here, so leave as-is; architect is instructed to copy it\n                        pass\n            except Exception:\n                pass\n            # Compute final path and update outputs.draft_file deterministically\n            P = _next_P()\n            final = Path(\"prp/drafts\") / f\"P-{P:03d}-T-002-Q-{qnum:03d}.json\"\n            try:\n                outs[\"draft_file\"] = str(final)\n            except Exception:\n                payload[\"outputs\"] = {\"draft_file\": str(final)}\n\n            if isinstance(content, dict):\n                final.parent.mkdir(parents=True, exist_ok=True)\n                Path(final).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n                print(f\"Saved draft -> {final}\")\n            else:\n                # invalid content; save diagnostics\n                out = Path(\"tmp/raw\") / f\"task002-Q-{qnum:03d}-invalid.json\"\n                _ensure_parent_dir(str(out))\n                out.write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n                print(f\"WARN: invalid JSON wrapper for Q-{qnum:03d}. Saved -> {out}\")\n\n        return 0\n\n    # Legacy steps mode\n    # Parse spec\n    try:\n        spec = parse_prp_steps(args.md)\n    except SpecError as e:\n        print(f\"SpecError: {e}\")\n        return 1\n\n    # Determine steps\n    # For backward compatibility, allow a single default step if not provided\n    available_ids = [s.id for s in spec.steps]\n    selected_ids = available_ids\n    runtime_args = {\"feature_description\": args.feature_description or \"\"}\n    _prompt_text = Path(args.prompt).read_text(encoding=\"utf-8\", errors=\"replace\") if Path(args.prompt).exists() else \"\"\n    requests = []\n    batch_ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    for sid in selected_ids:\n        step = next(s for s in spec.steps if s.id == sid)\n        agent_text = load_agent_text(step.agent)\n        resolved_inputs = _resolve_mapping(step.inputs, runtime_args)\n        if _prompt_text:\n            resolved_inputs[\"_prompt_text\"] = _prompt_text\n        if step.action == \"generate_from_template\":\n            resolved_inputs[\"template\"] = args.template\n        user_text = build_user_instruction_for_step(step, resolved_inputs)\n        req = {\n            \"custom_id\": f\"step-{sid}\",\n            \"params\": {\n                \"model\": args.model,\n                \"max_tokens\": int(args.max_tokens),\n                \"temperature\": 0.2,\n                \"system\": [\n                    {\"type\": \"text\", \"text\": agent_text, \"cache_control\": {\"type\": \"ephemeral\", \"ttl\": \"1h\"}}\n                ],\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_text}]}\n                ],\n            },\n        }\n        requests.append(req)\n\n    batch = client.messages.batches.create(requests=requests)\n    print(f\"batch_id={batch.id} status={batch.processing_status} count={len(requests)}\")\n    while True:\n        b = client.messages.batches.retrieve(batch.id)\n        print(f\"poll: status={b.processing_status}\")\n        if b.processing_status in (\"ended\", \"failed\", \"expired\"):\n            break\n        time.sleep(2)\n    items = list(client.messages.batches.results(batch.id))\n    print(f\"results_count={len(items)}\")\n    if not items:\n        print(\"No results returned.\")\n        return 3\n\n    for it in items:\n        blocks = _extract_text_blocks_from_result(it)\n        cid = _get_custom_id(it)\n        step_id = cid.replace(\"step-\", \"\") if cid else \"unknown-step\"\n        if not blocks:\n            print(f\"WARN: no text blocks for {cid or 'unknown'}; raw=\\n{it}\")\n            continue\n        combined = \"\\n\\n\".join(blocks)\n        payload = _extract_first_json_object(combined) or _extract_fenced_json(combined)\n        if isinstance(payload, str):\n            try:\n                payload = json.loads(payload)\n            except Exception:\n                pass\n        feature = args.feature_description or \"\"\n        slug = _slugify(feature)\n        if not isinstance(payload, dict):\n            raw_out = f\"tmp/raw/{slug}-{step_id}-{batch_ts}.txt\"\n            _ensure_parent_dir(raw_out)\n            Path(raw_out).write_text(combined, encoding=\"utf-8\")\n            print(f\"Saved raw -> {raw_out}\")\n            continue\n\n        def _is_valid(obj: dict) -> bool:\n            outs = obj.get(\"outputs\")\n            content = obj.get(\"content\")\n            return isinstance(outs, dict) and isinstance(outs.get(\"draft_file\"), str) and isinstance(content, dict)\n\n        if _is_valid(payload):\n            outputs = payload.get(\"outputs\", {})\n            dest = None\n            if isinstance(outputs, dict) and isinstance(outputs.get(\"draft_file\"), str):\n                dest = outputs[\"draft_file\"].replace(\"{slug}\", slug).replace(\"{timestamp}\", batch_ts).replace(\"{variant}\", \"a\").replace(\"{prp_id}\", \"000\")\n            if not dest:\n                dest = f\"{slug}-{step_id}-{batch_ts}.json\"\n            dest = _normalize_draft_path(dest, slug, step_id, batch_ts, \"json\")\n            _ensure_parent_dir(dest)\n            Path(dest).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n            print(f\"Saved draft -> {dest}\")\n            continue\n\n        if isinstance(payload, dict) and (\"proposed_tasks\" in payload or \"atomicity\" in payload):\n            out_path = f\"tmp/panel/{slug}-{step_id}-{batch_ts}.json\"\n            _ensure_parent_dir(out_path)\n            Path(out_path).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n            print(f\"Saved panel tasks -> {out_path}\")\n            continue\n\n        if isinstance(payload, dict):\n            raw_out = f\"tmp/raw/{slug}-{step_id}-{batch_ts}-invalid.json\"\n            _ensure_parent_dir(raw_out)\n            Path(raw_out).write_text(json.dumps(payload, indent=2), encoding=\"utf-8\")\n            print(f\"WARN: invalid JSON payload for {step_id}. Saved diagnostics -> {raw_out}\")\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n\n","numLines":1068,"startLine":1,"totalLines":1068}}}
